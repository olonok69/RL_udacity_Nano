{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89e40ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "tqdm.monitor_interval = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a126bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c68af2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 10, False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3eb2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bec8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23, 10, False), -1.0, True, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26245e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 3, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a52991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f155aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):\n",
    "            p[action] = 1 / env.action_space.n\n",
    "        policy[key] = p\n",
    "    return policy\n",
    "\n",
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q\n",
    "\n",
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "\n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "            top_range += prob[1]\n",
    "            if n < top_range:\n",
    "                action = prob[0]\n",
    "                break \n",
    "        state[0], reward, finished, info = env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "\n",
    "        episode.append(timestep)\n",
    "\n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(1)\n",
    "    return episode\n",
    "\n",
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "            wins += 1\n",
    "    return wins / r\n",
    "\n",
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
    "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {} # 3.\n",
    "    \n",
    "    for _ in range(episodes): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        \n",
    "        for i in reversed(range(0, len(episode))):   \n",
    "            s_t, a_t, r_t = episode[i] \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q # 14.\n",
    "                \n",
    "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bb5c27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tuple' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-57001fc159c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonte_carlo_e_soft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-9b107987c0c4>\u001b[0m in \u001b[0;36mmonte_carlo_e_soft\u001b[1;34m(env, episodes, policy, epsilon)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmonte_carlo_e_soft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Create an empty dictionary to store state action values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_state_action_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Empty dictionary for storing rewards for each state-action pair\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mreturns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;31m# 3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-9b107987c0c4>\u001b[0m in \u001b[0;36mcreate_random_policy\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_random_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mcurrent_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tuple' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "policy = monte_carlo_e_soft(env, episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f8462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fb5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0c70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4d180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29614a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f20e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696cce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a519d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b879c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(Qsa, Qsa_next, reward, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent time step \"\"\"\n",
    "    return Qsa + (alpha * (reward + (gamma * Qsa_next) - Qsa))\n",
    "\n",
    "def epsilon_greedy_probs(env, Q_s, i_episode, eps=None):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    epsilon = 1.0 / i_episode\n",
    "    if eps is not None:\n",
    "        epsilon = eps\n",
    "    policy_s = np.ones(env.nA) * epsilon / env.nA\n",
    "    policy_s[np.argmax(Q_s)] = 1 - epsilon + (epsilon / env.nA)\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5451a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()   \n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode, observe S\n",
    "        state = env.reset()   \n",
    "        # get epsilon-greedy action probabilities\n",
    "        policy_s = epsilon_greedy_probs(env, Q[state], i_episode)\n",
    "        # pick action A\n",
    "        action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "        # limit number of time steps per episode\n",
    "        for t_step in np.arange(300):\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            if not done:\n",
    "                # get epsilon-greedy action probabilities\n",
    "                policy_s = epsilon_greedy_probs(env, Q[next_state], i_episode)\n",
    "                # pick next action A'\n",
    "                next_action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "                # update TD estimate of Q\n",
    "                Q[state][action] = update_Q(Q[state][action], Q[next_state][next_action], \n",
    "                                            reward, alpha, gamma)\n",
    "                # S <- S'\n",
    "                state = next_state\n",
    "                # A <- A'\n",
    "                action = next_action\n",
    "            if done:\n",
    "                # update TD estimate of Q\n",
    "                #print(next_state)\n",
    "                Q[state][action] = update_Q(Q[state][action], 0, reward, alpha, gamma)\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa26c4b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BlackjackEnv' object has no attribute 'nA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d6d9aff0df5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# obtain the estimated optimal policy and corresponding action-value function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mQ_sarsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-3d1118675c58>\u001b[0m in \u001b[0;36msarsa\u001b[1;34m(env, num_episodes, alpha, gamma)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# get epsilon-greedy action probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mpolicy_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_greedy_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_episode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m# pick action A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-3d1118675c58>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# initialize action-value function (empty dictionary of arrays)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# initialize performance monitor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplot_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BlackjackEnv' object has no attribute 'nA'"
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa = sarsa(env, 5000, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9840c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d45109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  4,  4,  4,  3, -1,  2,  2,  3,  0, -1,  4,  1,  0,  1, -1,  5,\n",
       "        0,  0,  0, -1,  3,  3,  3,  2, -1,  1,  3,  0,  0, -1,  2,  0,  3,\n",
       "        1, -1,  3,  0,  0,  0, -1,  2,  2,  2,  0, -1,  3,  1,  3,  0, -1,\n",
       "        0,  0,  2,  0, -1,  0,  2,  0,  0, -1,  3,  2,  1,  2, -1,  3,  2,\n",
       "        1,  2, -1,  3,  3,  0,  1, -1,  0,  2,  0,  0, -1,  3,  2,  0,  4,\n",
       "       -1,  4,  4,  0,  1, -1,  3,  2,  3,  1, -1,  3,  5,  3,  0, -1,  1,\n",
       "        1,  1,  1, -1,  1,  2,  3,  3, -1,  2,  1,  0,  3, -1,  1,  0,  0,\n",
       "        0, -1,  3,  3,  3,  3, -1,  1,  2,  2,  1, -1,  0,  2,  2,  1, -1,\n",
       "        3,  0,  0,  0, -1,  1,  0,  1,  3, -1,  1,  3,  1,  2, -1,  0,  2,\n",
       "        2,  1, -1,  0,  2,  0,  0, -1,  1,  0,  0,  2, -1,  5,  3,  0,  0,\n",
       "       -1,  3,  1,  2,  2, -1,  0,  2,  0,  0, -1,  0,  0,  3,  3, -1,  1,\n",
       "        1,  3,  3, -1,  0,  3,  0,  3, -1,  3,  1,  3,  3, -1,  1,  1,  2,\n",
       "        0, -1,  3,  1,  0,  2, -1,  1,  1,  0,  1, -1,  1,  2,  0,  2, -1,\n",
       "        3,  1,  1,  2, -1,  2,  3,  3,  0, -1,  3,  3,  3,  0, -1,  3,  2,\n",
       "        3,  2, -1,  1,  2,  2,  3, -1,  0,  2,  4,  2, -1,  1,  0,  2,  3,\n",
       "       -1,  3,  2,  3,  2, -1,  1,  3,  2,  3, -1,  3,  2,  1,  2, -1,  0,\n",
       "        2,  0,  0, -1,  3,  2,  3,  0, -1,  1,  2,  3,  3, -1,  0,  3,  1,\n",
       "        0, -1,  0,  2,  4,  1, -1,  3,  1,  3,  3, -1,  0,  0,  0,  2, -1,\n",
       "        0,  3,  0,  0, -1,  0,  0,  2,  0, -1,  1,  1,  0,  1, -1,  2,  1,\n",
       "        1,  1, -1,  2,  0,  2,  2, -1,  0,  0,  2,  2, -1,  1,  0,  2,  2,\n",
       "       -1,  0,  2,  3,  3, -1,  3,  1,  3,  3, -1,  2,  3,  1,  0, -1,  0,\n",
       "        3,  0,  1, -1,  2,  0,  3,  1, -1,  2,  4,  0,  3, -1,  0,  0,  0,\n",
       "        0, -1,  1,  1,  1,  0, -1,  1,  0,  3,  0, -1,  0,  2,  3,  2, -1,\n",
       "        1,  3,  1,  0, -1,  2,  1,  0,  3, -1,  0,  0,  3,  2, -1,  0,  2,\n",
       "        4,  4, -1,  4,  0,  3,  3, -1,  1,  1,  5,  1, -1,  2,  0,  1,  5,\n",
       "       -1,  3,  0,  3,  0, -1,  1,  3,  3,  4, -1,  1,  3,  2,  0, -1,  3,\n",
       "        2,  3,  3, -1,  0,  2,  3,  0, -1,  0,  3,  0,  3, -1,  3,  0,  2,\n",
       "        3, -1,  0,  1,  0,  3, -1,  1,  1,  0,  0, -1,  0,  4,  4,  4, -1,\n",
       "        1,  1,  1,  5, -1,  0,  3,  0,  2, -1,  3,  1,  0,  3, -1,  0,  3,\n",
       "        3,  1, -1,  3,  1,  3,  3], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "800cb9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.31020292, -4.30972435, -4.31479837, -4.3151394 , -4.37754542,\n",
       "       -4.47654306])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_sarsa[268]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e5e14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode, observe S\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # get epsilon-greedy action probabilities\n",
    "            policy_s = epsilon_greedy_probs(env, Q[state], i_episode)\n",
    "            # pick next action A\n",
    "            action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            # update Q\n",
    "            Q[state][action] = update_Q(Q[state][action], np.max(Q[next_state]), \\\n",
    "                                                  reward, alpha, gamma)        \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # until S is terminal\n",
    "            if done:\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "838a7362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000/5000"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average Reward over 100 Episodes:  8.38\n"
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsamax = q_learning(env, 5000, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2d4aec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  4,  4,  0, -1,  0,  2,  0,  0, -1,  0,  2,  0,  0, -1,  5,\n",
       "        0,  0,  0, -1,  3,  3,  3,  0, -1,  0,  0,  3,  3, -1,  0,  0,  0,\n",
       "        0, -1,  3,  0,  2,  0, -1,  0,  0,  0,  2, -1,  2,  2,  0,  0, -1,\n",
       "        0,  2,  2,  0, -1,  0,  2,  2,  1, -1,  0,  0,  0,  2, -1,  2,  2,\n",
       "        0,  0, -1,  3,  0,  0,  0, -1,  0,  2,  0,  0, -1,  3,  0,  0,  4,\n",
       "        0,  4,  4,  3,  3, -1,  3,  3,  3,  3, -1,  0,  5,  0,  0, -1,  1,\n",
       "        1,  1,  2, -1,  2,  2,  0,  0, -1,  0,  0,  2,  0, -1,  1,  2,  0,\n",
       "        0, -1,  3,  3,  3,  0, -1,  0,  0,  0,  0, -1,  3,  0,  0,  0, -1,\n",
       "        3,  0,  1,  0, -1,  0,  0,  0,  2, -1,  2,  2,  0,  0, -1,  0,  2,\n",
       "        0,  0, -1,  0,  2,  0,  0, -1,  0,  0,  0,  2, -1,  2,  2,  0,  3,\n",
       "       -1,  3,  0,  0,  0, -1,  0,  1,  0,  0, -1,  3,  0,  0,  1, -1,  1,\n",
       "        1,  0,  3, -1,  0,  3,  3,  3, -1,  3,  1,  3,  3, -1,  1,  1,  1,\n",
       "        2, -1,  2,  2,  0,  0, -1,  0,  2,  2,  2, -1,  1,  2,  0,  2, -1,\n",
       "        1,  1,  1,  2, -1,  2,  2,  3,  3, -1,  3,  2,  2,  2, -1,  1,  2,\n",
       "        3,  2, -1,  3,  3,  3,  2, -1,  2,  1,  3,  3, -1,  3,  2,  2,  2,\n",
       "       -1,  3,  2,  3,  2, -1,  3,  3,  3,  1, -1,  1,  2,  3,  3, -1,  3,\n",
       "        0,  0,  0, -1,  3,  2,  3,  0, -1,  3,  3,  3,  1, -1,  1,  1,  3,\n",
       "        3, -1,  3,  3,  0,  3, -1,  3,  1,  3,  3, -1,  1,  1,  1,  1, -1,\n",
       "        1,  1,  0,  0, -1,  0,  1,  1,  1, -1,  1,  1,  0,  1, -1,  1,  1,\n",
       "        1,  1, -1,  2,  1,  1,  1, -1,  1,  1,  1,  1, -1,  3,  3,  0,  1,\n",
       "       -1,  1,  3,  1,  1, -1,  1,  1,  1,  1, -1,  3,  1,  1,  1, -1,  2,\n",
       "        1,  3,  2, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  0,  0,\n",
       "        0, -1,  1,  1,  1,  0, -1,  3,  1,  1,  1, -1,  1,  1,  1,  1, -1,\n",
       "        3,  0,  3,  3, -1,  3,  1,  2,  3, -1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        4,  4,  0,  4,  1,  1,  1, -1,  1,  1,  5,  1, -1,  1,  1,  1,  1,\n",
       "       -1,  2,  1,  1,  1, -1,  1,  1,  1,  1, -1,  3,  0,  2,  0, -1,  1,\n",
       "        3,  1,  1, -1,  1,  1,  1,  3, -1,  3,  1,  1,  1, -1,  2,  0,  0,\n",
       "        3, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  4,  4,  4,  0,\n",
       "        1,  1,  1,  5, -1,  3,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  3,\n",
       "        3,  3, -1,  0,  1,  2,  3], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the estimated optimal policy\n",
    "policy_sarsamax = np.array([np.argmax(Q_sarsamax[key]) if key in Q_sarsamax else -1 for key in np.arange(500)])\n",
    "policy_sarsamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b48f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode\n",
    "        state = env.reset()\n",
    "        # get epsilon-greedy action probabilities\n",
    "        policy_s = epsilon_greedy_probs(env, Q[state], i_episode, 0.005)\n",
    "        while True:\n",
    "            # pick next action\n",
    "            action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            # get epsilon-greedy action probabilities (for S')\n",
    "            policy_s = epsilon_greedy_probs(env, Q[next_state], i_episode, 0.005)\n",
    "            # update Q\n",
    "            Q[state][action] = update_Q(Q[state][action], np.dot(Q[next_state], policy_s), \\\n",
    "                                                  reward, alpha, gamma)        \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # until S is terminal\n",
    "            if done:\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e54083ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/10000"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxkZX3v8c+3qpfpnn2FYYZhhs0IXEJkQsAt4hIw111UjAYTjQQvvrKaKFeNGuXeaEw0xgTBRKNxxbghURRUMNygOKPIvgybDDD70jPTWy2/+8c53RTdVafOdE9113R/369XvbrqOVV1fk939Xnq2RURmJmZTURhugMwM7PDlwsRMzObMBciZmY2YS5EzMxswlyImJnZhHVMdwCttmzZsli7du10h2FmdljZuHHjjohY3ux5M74QWbt2LRs2bJjuMMzMDiuSHs7zPDdnmZnZhLkQMTOzCXMhYmZmE9a0T0RSAfhV4ChgALgjIra2OjAzM2t/DQsRSccBbweeD9wHbAfmACdK6gcuBz4TEdWpCNTMzNpPVk3kA8BlwB/GmFUaJa0Afgf4XeAzrQvPzMzaWcNCJCJem3FsG/DRlkRkZmaHjTx9Iq8CromIfZLeBTwN+EBE/Kzl0c1guw4M88iufhb3dnHEwm66O4qZz48I9g6U6OkqNn3uiGo1eU3fYIm+gTL7hkoMDFcYKFUQYtXiHtYs6WVxbyflatA/VGGoXKGro8CcziLdHcm4i2pANYKOgpAEQKUa7OkfZnd/icFSheFKlVK5SkexQE9nkZ6uIsvndzOv++CmIg0MV/jlrn627xuiGpGet8Cy+V2smD+Hxb2dVANKlSpD5So79w+xfd8QewZKzO/uYPHcLhb3drGot5M5neN/TxHBnv4Sj+0doCCxqLeThT2dFCSGylWGyhUioFgQRYn9Q2Ue2zPAo3sGqAYcv2Iex6+Yx5yOAg/v6mfTtv08unuA4UqV4XKVzmKBM9Yt4VdXL6SjWBg9564Dw2ztG2LbvkF27B8GoLMoOgoFFs/tZMX8bpbO7WagVGHXgWF2HhimUq1SkCgWxPL53axdOrdunrLsTfPa1VGgu+bv2t1RpFgQ+wfL9A2WKFeDY5b0UijoSa/fvm+IgeEKpWqVciXoGyyxp7/Env5hOosF5s/pYF76e182r5tFPZ1IMFCqsH+wTDUYPXexICQQSj9Xyd935PddkOgoaFwMY/9+ewdKo5+zYkEMlirs7h9m70CJahU6ik+8V7EgOopiXncS58jnt9ZQucK+wTIDw5XRz0B3R5GjFs2ht6v+57dSTf6m+4fK7B8sJ/9TgoJEQU/8zwAsmNPJ4rmdLO7torM4fixTbUNPBETN7+bAUJK3Pf3DFAsFVszvZvn87nHvU60+8bcZKCX/44PDFc46bmndPB9Kef7D3x0RX5H0TOAc4MMkzVy/0dLIDnO70w9Y/3DyIbhv237u3bKP+7bt476t+9l5YPhJz182r4ueriKdxQJdxeSfrrOY/OPtPDDMY3sG6B+uUBCsXtzLumVzqUbw+N5BtuwdJCJYNr+bpXO7CGDr3kG27RuiXG2+X0yxICo5ngfJBaGzIPpLycW2maVzu1iztJfOQoFd/cPsPjBM/3CFSgSValAsiLldReZ2d1CqVNnaN5QrjryxLuzppKtYYOT/aFd6/snqKCjzdzu/u4OTjlrArgPDbN49wEBp8ucEWLWoh2XzkotRZ7HAULnCjv3D7Nyf/N6OXtLLmiW9FCTueHwvj+wayP3ei3s7Oeu4pZy6ehH3btnHTx7cxaN78r8eks8SkPvzVE9B0FEs0NtV5OjFvaxZ2svSuV1s2rafux7vY3d/afS5XcUCw5V83bKdRbGot4vOghiuBKVKlYHhSubrF/Z0jv5vzukoUolg695Btu4bmlAeJUYLt2okv6eJvM+87o70y0CBclqg1fs83v3+cw/6i8fBUrNNqST9PCJ+TdL/BW6LiC+MpLU0skNk/fr1MZUz1nfsH+KSr93GtXeOH8A2v7uD44+Yx4kr5nPCEfNYs6SXPf2lpCDoG2Qo/UY/XK6O/ixVqiyZ28WqRb0ctWgOfYNlHti+nwd3HKBYECsXzmHlwh4k2LF/mB37higU4IgFc1i5cA7L5nWzsKeTBXM6mdvdQW9XUkuoVIPNuwf45a5+du4foqezSG/6wRwuVxksVxgqVUe/XYn023+lSqkczJvTwZLeThbP7WJOZzEtXAqUq1UGSxX6hyts6Rvklzv7eXhnP9UIlsztYvHcLno7i3QUCxQLUK4GB4bKHBiqUJBYu7SXY5bN5Yj53XQUk5pPqVxl+/4htvYNsTf9RtZRFN0dBZbOG/kG3MX+oTJ7+ofZlX4r3dtfYu9A8i175BvvkrldHLWoh5UL5wCwJ31OEHR3JPkoKPlmV64GPZ1FVi3uYdWiHiSxadt+7t26j/1DZY5bPo8TViR/xzmdRTqLYt9gmZse2MmNm3Zw1+N9rJjfzdGLe1m1uIcjF8xhxYJuls3rRohSNfn77jowzPZ9Q+zYP0xvV5Elc7tYOreLjmJh9CKztW+QB3cc4IHt+9kzUEo+I+Uq3Z0Fls1LajHVCDbvTn7f5Wpw0lELOOWohRyztDf525WSv+twOanFlSvJ33H+nA4IuPmhXfz3ph08tneQZfO6OGPdEk4/ZgmLezuTb/SFpOaxuLeLhT2dlKtV9g+V2TdYZueBpCDbkRZm87o7mTeng6LEcDn5hl97kYsICmltD6ASQbUaVKpQriaf/wNDZX65a4Bf7jzAzv3DHLt8Lk9duYDjV8yjUg36hysMlivJN/209lkQo+8x8rsrV4P9g+XRLzGVatDZkXxZ6+kqMq87+R30pJ/j7o4Cg6Uqj+0d4PE9g+w8MMRgKflcS3DkguTzs3x+Nwt6OpjX3UlPZ5EgRmsgBSV5C4K+geTcew4MU6pUKaVx1da8aitfI7UZSfR2FUfzVqkG2/YNsbVvkL3pZ2CoXKUoPfF/0NtJb1eROZ1FejqLnH7M4tEa8cGStDEi1jd9Xo5C5GrgUZJRWqeTDPO9OSJ+dUKRTbGpLES+d8cWLvnabewbKnPhs47lmKW99HZ1sKCng+OWz2Plwjktr1qaTUZEsLu/xOLeTn9WZ7m8hUie5qxXA+cCH46IPZJWAn8x2QAnStK5wD8AReBfIuJvpiuWHz+wk8/e9BBb9g6ytW+IR/cM8NSVC/jCa07jKUfOn66wzCZMEkvmdk13GHYYaVqIRES/pG3AM0nmi5TTn1NOUhH4J+AFwGbgp5Kuiog7pyOej153L7c/2sepqxdyxrolnLRyAW94+lq6OrwQgJnNDnlGZ70HWA88Bfg00Al8DnhGa0Or6wxgU0Q8kMb2JeClwJQXIgeGymx8eDdvfOY6LnnhU6f69GZmbSHPV+aXAy8BDgBExGPAdLXVrAIeqXm8OU17EkkXStogacP27dtbEshN9++kVAl+84Smy+2bmc1YeQqR4XTGegBImtvakDLV6+kbNzIgIq6IiPURsX758tZc5G+4d3sy+mHt4pa8v5nZ4SBPIXKlpMuBRZLeDFwHfLK1YTW0GTi65vFq4LHpCORH923nrOOW5p74Z2Y2E+XpWP+wpBcAfST9In8VEde2PLL6fgqcIGkdybDj80nW8JpSD+88wMM7+3njM9ZN9anNzNpKrjUp0kJjugqO2jjKkt4KfJdkiO+nIuKOqY7jR/cm/SzPPtH9IWY2u2UtBb+POv0NIyJiQUsiaiIivg18ezrOPeKGe3dw9JIe1i7tnc4wzMymXdYqvvMBJP01sAX4d5KO7dcxfaOzpt1wucpN9+/g5U9b5Rm9Zjbr5elYPyci/jki9kVEX0RcBryy1YG1q40P7+bAcIVne2ivmVmuQqQi6XWSipIKkl4HHJolSQ9DP7pvOx0FcdZxS6c7FDOzaZenEPkdkvWztgLbgFcxDSOi2sXtj+7lpKMWMH9O53SHYmY27fIM8X2IZGkRA7b2DbJu2XTOtzQzax9NayKSVkv6uqRtkrZK+qqk1VMRXDva2jfEEQvmTHcYZmZtIU9z1qeBq4CjSNap+laaNusMlirsHSi5EDEzS+UpRJZHxKcjopze/g2YlUOTtvYNArgQMTNL5SlEdkh6fTo6qyjp9cDOVgfWjrbsTQqRI12ImJkB+QqRN5KMztqS3s5L02adLaM1ke5pjsTMrD3kGZ31S5L9RGa9bX1DAByx0DURMzPINzrrQ5IWSOqU9H1JO9ImrVlnS98gPZ1F5nfnWrfSzGzGy9Oc9VsR0Qe8iGQ/jxOBv2hpVG1qa98gRy6c4zWzzMxSeQqRkanZvw18MSJ2tTCetra1b5AV890fYmY2Ik8h8i1JdwPrge9LWg4Mtjas9rS1b4gj3R9iZjaqaSESEe8AzgLWR0QJOMAsXAYlItjSN+g5ImZmNbI2pXpuRPxA0itq0mqf8rVWBtZu9g6UGC5XXYiYmdXIGmb0m8APgBfXORbMskLEc0TMzMbL2tnwPenP35+6cNrX1nSOiGerm5k9Ic88kaWSPibpZ5I2SvoHSbNuR6ate71ulpnZWHlGZ30J2E6yJe556f0vtzKodjSy+OIKN2eZmY3KM/V6SUS8v+bxByS9rFUBtastfYMs7u2ku6M43aGYmbWNPDWRH0o6P91fvSDp1cB/tjqwduPNqMzMxstTiPwh8AVgKL19CfgzSfsk9bUyuHay1XNEzMzGybOK7/ypCKTdbe0b5Kkr/aswM6vVsCZSu1KvpGeMOfbWVgbVbsqVKjv2D3l4r5nZGFnNWX9Wc/8fxxybVZtS7dg/TDVghQsRM7MnySpE1OB+vceHjKT3SnpU0i3p7bdrjl0iaZOkeySd06oYxhqZre6aiJnZk2X1iUSD+/UeH2ofiYgP1yZIOgk4HzgZOAq4TtKJEVFpcSyjc0TcsW5m9mRZhcivSLqVpNZxXHqf9PGxLY9svJcCX4qIIeBBSZuAM4CbWn3i0UJkoScampnVyipEnjplUYz3VkkXABuAP4+I3cAq4Mc1z9mcpo0j6ULgQoA1a9ZMOpitfYMUC2LpXBciZma1shZgfLhVJ5V0HXBknUPvBC4D3k/SZPZ+4O9IOvLr9cPUbVaLiCuAKwDWr18/6aa3LXuHWDG/m2LB2+KamdXKs+zJIRcRz8/zPEmfBK5OH24Gjq45vBp47BCHVte2fYMemWVmVkeeGetTStLKmocvB25P718FnC+pW9I64ATg5qmI6cBQmfnd01Lempm1tXa8Mn5I0mkkTVUPkSy7QkTcIelK4E6gDFw8FSOzAEqVoKPopiwzs7GytsddCFwCvAxYniZvA74J/E1E7GlFQBHxuxnHLgUubcV5s5QqVToKbVdpMzObdllXxiuB3cBzImJpRCwFzk7TvjIVwbWLcjXo6nBNxMxsrKxCZG1EfDAitowkRMSWiPggMPlxs4eRsmsiZmZ1ZV0ZH5b0l5KOGEmQdISktwOPtD609uE+ETOz+rIKkdcAS4EbJO2WtBu4HlgCvHoKYmsb5WqVTtdEzMzGyZpsuBt4e3qb1cquiZiZ1ZU5xFfSr5CsWbWKZMjtY8BVEXHXFMTWNoYrVTqLromYmY2VtSnV20m2whXJpL6fpve/KOkdUxNeeyhXgg4veWJmNk5WTeRNwMkRUapNlPT3wB3A37QysHZSrlbp7HBNxMxsrKwrY5Vk346xVqbHZoWIoFQJOl0TMTMbJ6sm8ifA9yXdxxNDetcAxwOzZo/1SjVZBLjDfSJmZuNkjc66RtKJJBs/rSLpD9kM/HSq1qxqB+XRQsQ1ETOzsTJHZ0VElSdvBAWApHkRsb9lUbWRUiVpufM8ETOz8SZ6ZbzzkEbRxkoV10TMzBrJWsX3zxodAua1Jpz2U05rIu4TMTMbL+vK+H+AxcD8Mbd5TV43o5TSPpEu10TMzMbJ6hP5GfCNiNg49oCkP2hdSO1ltCbiPhEzs3GyCpHfB3Y2OLa+BbG0JfeJmJk1ljXE956MY1tbE077KVfT0VnuEzEzG8dXxibKIzURz1g3MxvHhUgTwxXXRMzMGml6ZZT0jDxpM1XZfSJmZg3l+Xr9jznTZqSyayJmZg1lTTY8C3g6sHzMxMMFQLHVgbWLkXkina6JmJmNkzXEt4tkYmEHySTDEX3Aea0Mqp14noiZWWNZQ3xvAG6Q9OWIuLv2mKRlLY+sTXieiJlZY3m+Xl8p6cyRB5JeCfx360JqL54nYmbWWOZS8KnXAZ+SdD3JTodLgee2Mqh2UhptznJNxMxsrKZfryPiNuBS4CLgbOCtEbF5MieV9CpJd0iqSlo/5tglkjZJukfSOTXpp0u6LT32MUlTclUfac5yTcTMbLw880T+lWSr3FNJ1tP6lqSLJ3ne24FXAD8ac66TgPOBk4FzgX+WNDIS7DLgQuCE9HbuJGPIpexCxMysoTxXxtuBsyPiwYj4LnAm8LTJnDQi7mqwNtdLgS9FxFBEPAhsAs6QtBJYEBE3RUQAnwVeNpkY8hrpE3HHupnZeHmasz4CrJH0/DRpmKRm0gqrgEdqHm9O01al98em1yXpQkkbJG3Yvn37pAIabc7yEF8zs3HyNGe9GfgP4PI0aTXwjRyvu07S7XVuL816WZ20yEivKyKuiIj1EbF++fLlzULN9MTOhq6JmJmNlWd01sXAGcBPACLiPkkrmr0oIp7f7Dl1bAaOrnm8GngsTV9dJ73lylXPEzEzayRPG81QRAyPPJDUQUYtYJKuAs6X1C1pHUkH+s0R8TiwT9KZ6aisC4BvtiiGJxkup/NE3JxlZjZOnivjDZL+N9Aj6QXAV4BvTeakkl4uaTNwFvCfkr4LEBF3AFcCdwLXABdHRCV92VuAfyHpbL8f+M5kYsirXK1SEBQ8T8TMbJw8zVnvAN4E3Ab8IfBtkov5hEXE14GvNzh2Kcm8lLHpG4BTJnPeiShXwsN7zcwaaFqIREQV+GR6m3VKLkTMzBrKWgr+hzTu+4iIeF5rQmov5WrVnepmZg1k1UTeViftTOAvgW2tCaf9lCrhZeDNzBrIWgp+48h9Sb8JvBvoBi6KiCnp1G4H5UrVG1KZmTWQ2SeSLoD4bmAQuDQifjglUbWRUsXNWWZmjWT1ifwUWA78LXBTmja6ZlZE/Kzl0bWBUjU8R8TMrIGsmsgBYD/JVriv5MlLjwSzZE+RpDnLhYiZWT1ZfSLPmcI42la5Em7OMjNrwF+xmyhVgw7XRMzM6vLVsYlypUqnlzwxM6srsxBR4uis58x0bs4yM2sssxBJdxFsunfITDbsjnUzs4byXB1/LOnXWx5JmypXq3S4OcvMrK48q/ieDVwk6SGSYb8iqaSc2srA2oVX8TUzayxPIfLClkfRxkpuzjIza6jp1TEiHibZsva56f3+PK+bKcpVd6ybmTXStDCQ9B7g7cAlaVIn8LlWBtVOyl7F18ysoTxXx5cDLyHpDyEiHgPmtzKodlLyKr5mZg3lKUSG06G+ASBpbmtDai9exdfMrLE8hciVki4HFkl6M3Ads2irXDdnmZk1lmeP9Q9LegHQBzwF+KuIuLblkbWJUrVKV4cLETOzepoWIpL+FPjKbCo4aiU1ETdnmZnVk+cr9gLgu5L+S9LFko5odVDtIiLSIb6uiZiZ1ZNnnsj7IuJk4GLgKOAGSde1PLI2UK4GgFfxNTNr4GC+Ym8DtgA7gRWtCae9lCtJIeKaiJlZfXkmG75F0vXA94FlwJtny7pZw5UqgOeJmJk1kGftrGOAP46IX7Q6mHZTTgsRd6ybmdWXp53me8Cz0k71sw/FSSW9StIdkqqS1tekr5U0IOmW9PaJmmOnS7pN0iZJH5PU8iv7aJ+Ih/iamdXV8OooaZWknwDvAY4FjgfeK+lmSasmed7bgVcAP6pz7P6IOC29XVSTfhlwIXBCejt3kjE0VRppzvJkQzOzurKasz4OXBYR/1abKOkC4J+Bl070pBFxV/peuZ4vaSWwICJuSh9/FngZ8J2JxpDHEx3rbs4yM6sn6yv2SWMLEICI+CzwKy2LCNZJ+rmkGyQ9K01bBWyuec7mNK0uSRdK2iBpw/bt2yccSLma9ol4dJaZWV1ZNZFivURJhUbHxjzvOuDIOofeGRHfbPCyx4E1EbFT0unANySdTLKb4ljR6NwRcQVwBcD69esbPq+ZUsXzRMzMsmQVIt+S9EngTyLiAIyu4PsR4NvN3jginn+wwUTEEDCU3t8o6X7gRJKax+qap64GHjvY9z9YI30iromYmdWXdXX8S2Av8LCkjZI2AA+RLMT4tlYEI2m5pGJ6/1iSDvQHIuJxYJ+kM9NRWRcAjWozh0zJfSJmZpka1kQiogS8TdK7SUZmCdgUEf2TPamklwP/CCwH/lPSLRFxDvBs4K8llYEKcFFE7Epf9hbg34Aekg71lnaqwxPzRLpcEzEzq6thISLpmRFxY0QMALfVOb6ApP/i9oM9aUR8Hfh6nfSvAl9t8JoNwCkHe67JGJkn4smGZmb1ZfWJvFLSh4BrgI3AdmAOSa3kbJKZ7H/e8ginkftEzMyyZTVn/amkxcB5wKuAlcAAcBdweUTcODUhTp+ReSJeO8vMrL7MtbMiYjfJVrizZjvcWqPzRDxj3cysLl8dMwy7JmJmlsmFSIay+0TMzDL56pjBfSJmZtmyhvi+IuuFEfG1Qx9OeylVRzalcllrZlZPVsf6i9OfK4CnAz9IH58NXA/M+EJkdBVfzxMxM6sra4jv7wNIuppkRd/H08crgX+amvCml+eJmJlly3N1XDtSgKS2kiyKOOON7mzoPhEzs7ry7LF+vaTvAl8kWX79fOCHLY2qTZTKnidiZpalaSESEW9NF0x8dpp0Rbr21YxXck3EzCxTZiGSbkB1a0ScQp0FE2e6cqVKR0G5t/E1M5ttMttpIqIK/ELSmimKp62Uq+G9RMzMMuTpE1kJ3CHpZuDASGJEvKRlUbWJUqVKp/tDzMwaylOIvK/lUbSpcsU1ETOzLHk61m+YikDaUbla9RwRM7MMTa+Q6b7mP5W0X9KwpIqkvqkIbrqVKkGnZ6ubmTWU52v2x4HXAveR7G/+B2najFeqVOnscE3EzKyRPH0iRMQmScWIqACflvTfLY6rLZQr4XWzzMwy5ClE+iV1Abeke64/DsxtbVjtoVSpegVfM7MMea6Qv5s+760kQ3yPBl7ZyqDaheeJmJlly1MTOQ7YHhF9zLLhvqVK1etmmZllyFOI/B7wCUk7gf9KbzdGxO5WBtYOypXwullmZhnyzBO5AEDSUcB5JHuJHJXntYe7ctU1ETOzLE0LAkmvB54F/A9gB8nw3v9qcVxtYbgS9HS5EDEzayTPFfKjwGnAJ4E/iogPRcRNkzmppL+VdLekWyV9XdKimmOXSNok6R5J59Skny7ptvTYxzQFS+uWK1VPNjQzy9C0EImIZcAbgTnApZJulvTvkzzvtcApEXEqcC9wCYCkk0g2vToZOBf4Z0nF9DWXARcCJ6S3cycZQ1NeO8vMLFueZU8WAGuAY4C1wEKgOpmTRsT3IqKcPvwxsDq9/1LgSxExFBEPApuAM9J93RdExE0REcBngZdNJoY8Sl47y8wsU57O8Rtrbh+PiM2HOIY3Al9O768iKVRGbE7TSun9sektVfbaWWZmmfKMzjoVQNLciDjQ7PkjJF0HHFnn0Dsj4pvpc94JlIHPj7ysXggZ6Y3OfSFJ0xdr1kx8P61yxTURM7MseUZnnQX8KzAPWCPpV4E/jIj/lfW6iHh+k/d9A/Ai4HlpExUkNYyja562GngsTV9dJ73Rua8ArgBYv359w8KmmVLV80TMzLLkHZ11DrATICJ+ATx7MieVdC7wduAlEdFfc+gq4HxJ3ZLWkXSg3xwRjwP70mXpBVwAfHMyMeThtbPMzLLlXcX3kTEjaiuTPO/HgW7g2vR9fxwRF0XEHZKuBO4kaea6OF05GOAtwL+RLEf/nfTWUskqvi5EzMwayVOIPCLp6UCkq/n+EXDXZE4aEcdnHLsUuLRO+gbglMmc92AlNRE3Z5mZNZLna/ZFwMUko6E2k0w8zOwPmSm8iq+ZWbY8o7N2AK8beSxpMUkhMq62MJNEBJWqm7PMzLI0vEJKOlrSFZKulvQmSb2SPgzcA6yYuhCnR6mSDOpyc5aZWWNZNZHPAjcAXyVZYuTHwB3AqRGxZQpim1blajIp3/NEzMwayypElkTEe9P735W0Ffj1iBhqfVjTr1QeqYm4EDEzaySzTyTt/xhpz9kC9EqaCxARu1oc27QqpTURN2eZmTWWVYgsBDby5CVHfpb+DODYVgXVDsppn4g71s3MGmtYiETE2imMo+2UKiN9Iq6JmJk14q/ZDZSrHp1lZtaMC5EGyiM1ETdnmZk15CtkA54nYmbWXK5CRNIzJf1+en95usLujDbSJ+IhvmZmjeXZHvc9JMu2X5ImdQKfa2VQ7cCTDc3MmstzhXw58BLgAEBEPAbMb2VQ7WC0Ocvb45qZNZSnEBlOdx4MSLbJbW1I7WF0nohrImZmDeW5Ql4p6XJgkaQ3A9cBn2xtWNOvVPU8ETOzZvIsBf9hSS8A+oCnAH8VEde2PLJpVh5tznJNxMyskbzb414LzPiCo1bZM9bNzJpqWohI2kfaH1JjL7AB+POIeKAVgU23YQ/xNTNrKk9N5O+Bx4AvkCzGeD5wJMnmVJ8CntOq4KZT2ZMNzcyayvM1+9yIuDwi9kVEX0RcAfx2RHwZWNzi+KaN54mYmTWX5wpZlfRqSYX09uqaY2ObuWYMzxMxM2suTyHyOuB3gW3A1vT+6yX1AG9tYWzT6omOdddEzMwayTPE9wHgxQ0O33how2kfI0vBe3SWmVljeUZnzQHeBJwMzBlJj4g3tjCuaVfyPBEzs6byXCH/nWQ01jnADcBqYF8rg2oHT6zi65qImVkjeQqR4yPi3cCBiPgM8D+B/9HasKbfSJ9I0R3rZmYN5SlESunPPZJOARYCaydzUkl/K+luSbdK+rqkRWn6WkkDkm5Jb5+oec3pkm6TtEnSxyS19OpeqgadRdHi05iZHdbyFCJXSFoMvAu4CrgT+OAkz3stcEpEnArcyxN7le/bt1YAAAq1SURBVADcHxGnpbeLatIvAy4ETkhv504yhkzlStVb45qZNZHZsS6pAPRFxG7gR8Cxh+KkEfG9moc/Bs5rEsdKYEFE3JQ+/izwMuA7hyKeekqV8MgsM7MmMr9qR0SV1s8FeSNPLgzWSfq5pBskPStNWwVsrnnO5jStZcrVqtfNMjNrIs/aWddKehvwZdLdDQEiYlfWiyRdRzKqa6x3RsQ30+e8EygDn0+PPQ6siYidkk4HviHpZJI1u8ZqOFte0oUkTV+sWbMmK8yGypWgw53qZmaZ8hQiI/NBLq5JC5o0bUXE87OOS3oD8CLgeenOiUTEEDCU3t8o6X7gRJKax+qal68mWRSy0bmvAK4AWL9+/YSWZhmuuCZiZtZMnhnr6w71SSWdC7wd+M2I6K9JXw7sioiKpGNJOtAfiIhdkvZJOhP4CXAB8I+HOq5a5Up4joiZWRNNv2pL6pX0LklXpI9PkPSiSZ7348B8kqay2qG8zwZulfQL4D+Ai2qazd4C/AuwCbifFnaqQ9In4nWzzMyy5WnO+jSwEXh6+ngz8BXg6omeNCKOb5D+VeCrDY5tAE6Z6DkPVsl9ImZmTeX5qn1cRHyIdNJhRAxQv6N7Rim7T8TMrKk8V8nhdNn3AJB0HGnn90xWrnqeiJlZM3mas94LXAMcLenzwDOA32thTG2hVKl6BV8zsybyjM76nqSNwJkkzVh/HBE7Wh7ZNCtVgjmdLkTMzLLk2U/kKuCLwFURcaDZ82eKcqVKR3eeipqZ2eyV56v23wHPAu6U9BVJ56UbVc1oJc8TMTNrKk9z1g3ADZKKwHOBNwOfAha0OLZpVa56FV8zs2Zytdeko7NeDLwGeBrwmVYG1Q6edcJyVi6c8RUuM7NJydMn8mXgN0hGaP0TcH26uu+M9u4XnTTdIZiZtb28M9Z/JyIqAJKeIel3IuLiJq8zM7MZLk+fyDWSTpP0WpLmrAeBr7U8MjMza3sNCxFJJwLnA68FdpLsJ6KIOHuKYjMzszaXVRO5G/gv4MURsQlA0p9OSVRmZnZYyBrD+kpgC/BDSZ+U9DxmwcKLZmaWX8NCJCK+HhGvAX4FuB74U+AISZdJ+q0pis/MzNpY09l0EXEgIj4fES8i2Zb2FuAdLY/MzMza3kFNyY6IXRFxeUQ8t1UBmZnZ4UMRMd0xtJSk7cDDE3z5MmDGr1g8xmzMM8zOfM/GPMPszPdE8nxMRCxv9qQZX4hMhqQNEbF+uuOYSrMxzzA78z0b8wyzM9+tzLNXGDQzswlzIWJmZhPmQiTbFdMdwDSYjXmG2Znv2ZhnmJ35blme3SdiZmYT5pqImZlNmAsRMzObMBcidUg6V9I9kjZJOqxn50s6WtIPJd0l6Q5Jf5ymL5F0raT70p+La15zSZr3eySdU5N+uqTb0mMfk9T2a6lJKkr6uaSr08czOt+SFkn6D0l3p3/zs2Z6niFZHDb9fN8u6YuS5szEfEv6lKRtkm6vSTtk+ZTULenLafpPJK1tGlRE+FZzA4rA/cCxQBfwC+Ck6Y5rEvlZCTwtvT8fuBc4CfgQ8I40/R3AB9P7J6V57gbWpb+LYnrsZuAskoU4vwO8cLrzlyP/fwZ8Abg6fTyj802ydfUfpPe7gEWzIM+rSPY56kkfXwn83kzMN/Bski3Kb69JO2T5BP4X8In0/vnAl5vGNN2/lHa7pb/Y79Y8vgS4ZLrjOoT5+ybwAuAeYGWathK4p15+ge+mv5OVwN016a8FLp/u/DTJ62rg+8BzawqRGZtvYEF6MdWY9Bmb5zS+VcAjwBKS7S2uBn5rpuYbWDumEDlk+Rx5Tnq/g2SWu7LicXPWeCMfyBGb07TDXlo1/TXgJ8AREfE4QPpzRfq0Rvlfld4fm97OPgr8JVCtSZvJ+T4W2A58Om3C+xdJc5nZeSYiHgU+DPwSeBzYGxHfY4bnu8ahzOfoayKiDOwFlmad3IXIePXaQA/7cdCS5gFfBf4kIvqynlonLTLS25KkFwHbImJj3pfUSTvc8t1B0tRxWUT8GnCA7BW3Z0KeSfsAXkrSZHMUMFfS67NeUiftsMt3DhPJ50H/DlyIjLcZOLrm8WrgsWmK5ZCQ1ElSgHw+Ir6WJm+VtDI9vhLYlqY3yv/m9P7Y9Hb1DOAlkh4CvgQ8V9LnmNn53gxsjoifpI//g6RQmcl5Bng+8GBEbI+IEvA14OnM/HyPOJT5HH2NpA5gIbAr6+QuRMb7KXCCpHWSukg6l66a5pgmLB118a/AXRHx9zWHrgLekN5/A0lfyUj6+ekojXXACcDNaTV5n6Qz0/e8oOY1bSciLomI1RGxluRv+IOIeD0zON8RsQV4RNJT0qTnAXcyg/Oc+iVwpqTeNN7nAXcx8/M94lDms/a9ziP5v8mujU13J1E73oDfJhnFdD/wzumOZ5J5eSZJdfRWkg3Fbknzt5Sk0/m+9OeSmte8M837PdSMTgHWA7enxz5Okw63drkBz+GJjvUZnW/gNGBD+vf+BrB4puc5jfd9wN1pzP9OMiJpxuUb+CJJv0+JpNbwpkOZT2AO8BVgE8kIrmObxeRlT8zMbMLcnGVmZhPmQsTMzCbMhYiZmU2YCxEzM5swFyJmZjZhLkRsxpJUkXRLzS1zRWZJF0m64BCc9yFJyw7i+ddL2lDzeL2k6ycbR/pevyfp44fivczq6ZjuAMxaaCAiTsv75Ij4RCuDaWKFpBdGxHemMYZxJBUjojLdcVj7ck3EZp20pvBBSTent+PT9PdKelt6/48k3SnpVklfStOWSPpGmvZjSaem6UslfS9d9PByatYfkvT69By3SLpcUrFBWH8LvKtOrE+qSUi6WtJz0vv703xslHSdpDPSWs0Dkl5S8zZHS7om3VPiPc1iS9/3ryX9hGTVV7OGXIjYTNYzpjnrNTXH+iLiDJLZuh+t89p3AL8WEacCF6Vp7wN+nqb9b+Czafp7gBsjWfTwKmANgKSnAq8BnpHWiCrA6xrEehMwJOnsg8jfXOD6iDgd2Ad8gGSZ/5cDf13zvDPS854GvCptLsuKbS7JUuO/ERE3HkQ8Ngu5OctmsqzmrC/W/PxIneO3Ap+X9A2S5UMgWULmlQAR8YO0BrKQZKOgV6Tp/ylpd/r85wGnAz9NliiihycWx6vnAyS1kbfnyBvAMHBNev82YCgiSpJuI9lzYsS1EbETQNLX0nyUM2KrkCzYadaUCxGbraLB/RH/k6RweAnwbkknk71Mdr33EPCZiLgkV0BJwfR+4Mya5DJPbjGYU3O/FE+sW1QFhtL3qaYrsI6NsfZxVmyD7gexvNycZbPVa2p+3lR7QFIBODoifkiyqdUiYB7wI9Imn7RfYkcke7PUpr+QZNFDSBbDO0/SivTYEknHNInr0vScIx4CTpNUkHQ0SdPUwXpBeu4e4GXA/5tgbGbjuCZiM1mPpFtqHl8TESPDfLvTjuMCyfagtYrA59KmKgEfiYg9kt5LsmvgrUA/TyyZ/T7gi5J+BtxAsjQ5EXGnpHcB30sLphJwMfBwo4Aj4tuSttck/T+SLW9vI1l19WcH9RtI3Eiysu3xwBciYgPAwcZmVo9X8bVZR8lGVesjYsd0x2J2uHNzlpmZTZhrImZmNmGuiZiZ2YS5EDEzswlzIWJmZhPmQsTMzCbMhYiZmU3Y/wdQtzrc4OtKwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average Reward over 100 Episodes:  8.24\n"
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_expsarsa = expected_sarsa(env, 10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d3a2c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  4,  4,  0, -1,  0,  2,  0,  2, -1,  0,  2,  0,  0, -1,  5,\n",
       "        0,  0,  2, -1,  3,  3,  3,  0, -1,  0,  0,  3,  0, -1,  0,  0,  0,\n",
       "        0, -1,  3,  3,  2,  0, -1,  0,  0,  0,  2, -1,  2,  2,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  2,  0,  3, -1,  0,  0,  0,  2, -1,  2,  2,\n",
       "        3,  0, -1,  3,  3,  0,  0, -1,  3,  2,  0,  3, -1,  0,  0,  0,  4,\n",
       "        0,  4,  4,  3,  3, -1,  3,  3,  0,  0, -1,  3,  5,  0,  0, -1,  1,\n",
       "        1,  1,  2, -1,  0,  2,  0,  0, -1,  0,  2,  0,  2, -1,  1,  0,  0,\n",
       "        2, -1,  1,  3,  3,  0, -1,  0,  0,  0,  0, -1,  3,  0,  0,  0, -1,\n",
       "        3,  0,  0,  0, -1,  0,  0,  0,  2, -1,  2,  1,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  2,  0,  2, -1,  0,  3,  0,  2, -1,  2,  1,  3,  3,\n",
       "       -1,  3,  0,  0,  0, -1,  0,  1,  0,  0, -1,  3,  3,  3,  1, -1,  1,\n",
       "        1,  0,  1, -1,  3,  3,  0,  0, -1,  3,  1,  0,  0, -1,  1,  1,  1,\n",
       "        2, -1,  2,  2,  0,  0, -1,  0,  2,  2,  2, -1,  1,  2,  0,  2, -1,\n",
       "        1,  1,  1,  2, -1,  2,  2,  3,  3, -1,  3,  2,  2,  2, -1,  1,  2,\n",
       "        3,  2, -1,  3,  3,  3,  2, -1,  2,  2,  3,  3, -1,  3,  2,  2,  2,\n",
       "       -1,  3,  2,  3,  2, -1,  3,  3,  3,  1, -1,  2,  2,  3,  1, -1,  3,\n",
       "        0,  0,  0, -1,  3,  1,  3,  0, -1,  3,  3,  3,  1, -1,  1,  1,  3,\n",
       "        3, -1,  3,  0,  3,  3, -1,  3,  3,  3,  3, -1,  1,  1,  1,  1, -1,\n",
       "        1,  1,  0,  0, -1,  0,  1,  1,  1, -1,  1,  1,  0,  1, -1,  1,  1,\n",
       "        1,  1, -1,  1,  2,  1,  1, -1,  1,  2,  2,  1, -1,  1,  2,  1,  1,\n",
       "       -1,  3,  3,  1,  1, -1,  1,  1,  1,  1, -1,  3,  1,  1,  1, -1,  1,\n",
       "        1,  3,  1, -1,  1,  1,  1,  2, -1,  1,  2,  1,  1, -1,  1,  0,  0,\n",
       "        0, -1,  1,  1,  1,  0, -1,  3,  1,  3,  1, -1,  1,  1,  1,  3, -1,\n",
       "        3,  0,  3,  3, -1,  1,  3,  2,  3, -1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        4,  4,  0,  4,  1,  1,  1, -1,  1,  1,  5,  1, -1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  2,  1,  1, -1,  1,\n",
       "        3,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  0,  1,  0,\n",
       "        0, -1,  1,  1,  1,  1, -1,  1,  2,  1,  1, -1,  1,  4,  4,  4,  0,\n",
       "        1,  1,  1,  5, -1,  3,  1,  3,  1, -1,  1,  1,  1,  1, -1,  3,  3,\n",
       "        3,  3, -1,  1,  1,  1,  3], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the estimated optimal policy\n",
    "policy_expsarsa = np.array([np.argmax(Q_expsarsa[key]) if key in Q_expsarsa else -1 for key in np.arange(500)])\n",
    "policy_expsarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a9f5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \n",
    "        overwrite the current returned on the state\n",
    "    \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        # Constant-alpha\n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "095a153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    epsilon = eps_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode ##### Incremental Mean ###\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42ef6b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000."
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy, Q = mc_control(env, 500000, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e561e5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af942d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "       -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the estimated optimal policy\n",
    "policy = np.array([np.argmax(policy[key]) if key in policy else -1 for key in np.arange(500)])\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19fd5c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bfdb73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5080efd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "585579de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71db7e",
   "metadata": {},
   "source": [
    "### APPLY Expected SARSA POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "645fae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "action = env.reset()\n",
    "done = False\n",
    "final_reward=0\n",
    "while done==False:\n",
    "    action=policy_expsarsa[action]\n",
    "    action, reward, done, _ = env.step(action)\n",
    "    final_reward = final_reward + reward\n",
    "    env.render()\n",
    "    sleep(1)\n",
    "print(final_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255c314",
   "metadata": {},
   "source": [
    "### APPLY Q-LEARNING POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bc0d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# policy_sarsamax \n",
    "action = env.reset()\n",
    "done = False\n",
    "final_reward=0\n",
    "while done==False:\n",
    "    action=policy_sarsamax[action]\n",
    "    action, reward, done, _ = env.step(action)\n",
    "    final_reward = final_reward + reward\n",
    "    env.render()\n",
    "    sleep(1)\n",
    "print(final_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217342d",
   "metadata": {},
   "source": [
    "### APPLY SARSA POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33b28840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# policy_sarsa\n",
    "action = env.reset()\n",
    "done = False\n",
    "final_reward=0\n",
    "while done==False:\n",
    "    action=policy_sarsa[action]\n",
    "    action, reward, done, _ = env.step(action)\n",
    "    final_reward = final_reward + reward\n",
    "    env.render()\n",
    "    sleep(1)\n",
    "print(final_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7607910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_sarsa\n",
    "# action = env.reset()\n",
    "# done = False\n",
    "# final_reward=0\n",
    "# while done==False:\n",
    "#     action=policy[action]\n",
    "#     action, reward, done, _ = env.step(action)\n",
    "#     final_reward = final_reward + reward\n",
    "#     env.render()\n",
    "#     sleep(1)\n",
    "# print(final_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34729f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
