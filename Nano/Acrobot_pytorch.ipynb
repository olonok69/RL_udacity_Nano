{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0871bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import collections\n",
    "import gym\n",
    "import math\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4df9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Acrobot-v1'\n",
    "NETWORK_HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_BUFFER_CAPACITY = 500000\n",
    "SYNC_NETWORKS_EVERY_STEP = 1000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "DESIRED_TARGET_REWARD = -50\n",
    "N_DISCOUNT_STEPS = 2\n",
    "PRIORITY_ALPHA = 0.6\n",
    "PRIORITY_BETA_START = 0.4\n",
    "PRIORITY_BETA_GROW_LAST_STEP = 200000\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cef1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, initial_std=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=True)\n",
    "        w = torch.full((out_features, in_features), initial_std)\n",
    "        self.sigma_weight = nn.Parameter(w, requires_grad=True)\n",
    "        we = torch.zeros((out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', we)\n",
    "        if bias is not None:\n",
    "            b = torch.full((out_features,), initial_std)\n",
    "            self.sigma_bias = nn.Parameter(b, requires_grad=True)\n",
    "            be = torch.zeros(out_features)\n",
    "            self.register_buffer(\"bias_epsilon\", be)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Recommended Initialization by Ref.\"\"\"\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight_epsilon.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.bias_epsilon.normal_()\n",
    "            bias = bias + self.sigma_bias * self.bias_epsilon.data\n",
    "        weight = self.weight + self.sigma_weight * self.weight_epsilon.data\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_size, hidden_size, action_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        mid_size = int(hidden_size / 2)\n",
    "        self.common = nn.Sequential(\n",
    "            NoisyLinear(observation_size, 250),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(250, 200),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(200, mid_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(mid_size, action_size)\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(mid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.common(x)\n",
    "        advantage, value = self.advantage(x), self.value(x)\n",
    "        return value + (advantage - advantage.mean())\n",
    "\n",
    "\n",
    "class EpisodeSteps:\n",
    "    Step = collections.namedtuple('Step', field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "\n",
    "    def __init__(self, discount_steps=4):\n",
    "        self.discount_steps = discount_steps\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.done = None\n",
    "        self.next_state = None\n",
    "        self._steps = []\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self._steps.append(self.Step(state=state, action=action, reward=reward, done=done, next_state=next_state))\n",
    "\n",
    "    def roll_out(self, discount_factor):\n",
    "        \"\"\"Perform n-step roll outs\n",
    "        \"\"\"\n",
    "        first_step_total_discounted_rewards = self._discounted_rewards(discount_factor)\n",
    "        self._collapse_n_steps(first_step_total_discounted_rewards)\n",
    "\n",
    "    def _discounted_rewards(self, discount_factor):\n",
    "        total_discounted_reward_first_state = 0\n",
    "        for step in reversed(self._steps):\n",
    "            total_discounted_reward_first_state = step.reward + total_discounted_reward_first_state * discount_factor\n",
    "        return total_discounted_reward_first_state\n",
    "\n",
    "    def _collapse_n_steps(self, total_discounted_reward):\n",
    "        \"\"\"Collapses n-step into a single step and assigns the the calculated\n",
    "           total discounted reward to the first state.\n",
    "           We add the next_state observed in the last step as a next_state.\n",
    "        \"\"\"\n",
    "        self.state = self._steps[0].state\n",
    "        self.action = self._steps[0].action\n",
    "        self.reward = total_discounted_reward\n",
    "        self.done = self._steps[-1].done\n",
    "        self.next_state = self._steps[-1].next_state\n",
    "\n",
    "    def completed(self):\n",
    "        \"\"\"If episodes ends before reaching n-steps, (i.e done=True)\n",
    "           we consider the n-steps to be completed to avoid appending\n",
    "           an irrelevant next_state from the new episode to our last step.\n",
    "        \"\"\"\n",
    "        if self._steps[-1].done:\n",
    "            return True\n",
    "        return len(self._steps) == self.discount_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._steps)\n",
    "\n",
    "\n",
    "class PriorityReplayBuffer:\n",
    "    def __init__(self, capacity, priority_alpha=0.6, priority_beta_start=0.4, priority_beta_grow_last_step=10000,\n",
    "                 device='cpu'):\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "        self.priority_alpha = priority_alpha\n",
    "        self.priority_beta_start = priority_beta_start\n",
    "        self.priority_beta_grow_last_step = priority_beta_grow_last_step\n",
    "        self.device = device\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        self.priorities = np.zeros(self.capacity)\n",
    "\n",
    "    def append(self, episode_step):\n",
    "        max_priority = self.get_max_priority()\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(episode_step)\n",
    "        else:\n",
    "            self.buffer[self.position] = episode_step\n",
    "\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def get_max_priority(self):\n",
    "        if not self.buffer:\n",
    "            return 1\n",
    "        return np.array(self.priorities).max(initial=1)\n",
    "\n",
    "    def update_priorities(self, indexes, priorities):\n",
    "        for idx, priority in zip(indexes, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        probabilities = self.calculate_probabilities()\n",
    "        indexes = np.random.choice(len(self.buffer), sample_size, p=probabilities)\n",
    "        samples = [self.buffer[idx] for idx in indexes]\n",
    "\n",
    "        beta = self.update_beta()\n",
    "        samples_probabilities = probabilities.take(indexes)\n",
    "        sample_weights = (self._items_count() * samples_probabilities) ** (-beta)\n",
    "        sample_weights /= sample_weights.max()\n",
    "        return self._vectorize(samples, indexes, sample_weights)\n",
    "\n",
    "    def calculate_probabilities(self):\n",
    "        priorities = self.priorities[:self._items_count()]\n",
    "        priorities = priorities ** self.priority_alpha\n",
    "        return priorities / priorities.sum()\n",
    "\n",
    "    def update_beta(self):\n",
    "        step = len(self.buffer)\n",
    "        beta = self.priority_beta_start + (1 - self.priority_beta_start) * (step / self.priority_beta_grow_last_step)\n",
    "        return min(1, beta)\n",
    "\n",
    "    def _items_count(self):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            return self.position\n",
    "        return self.capacity\n",
    "\n",
    "    def _vectorize(self, samples, indexes, weights):\n",
    "        states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "        for episode_step in samples:\n",
    "            states.append(episode_step.state)\n",
    "            actions.append(episode_step.action)\n",
    "            rewards.append(episode_step.reward)\n",
    "            dones.append(episode_step.done)\n",
    "            next_states.append(episode_step.next_state)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states, copy=False)).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states, copy=False)).to(self.device)\n",
    "        actions = torch.LongTensor(np.array(actions, copy=False)).to(self.device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards, copy=False)).to(self.device)\n",
    "        dones = torch.BoolTensor(np.array(dones, copy=False)).to(self.device)\n",
    "        weights = torch.FloatTensor(np.array(weights, copy=False)).to(self.device)\n",
    "        return states, actions, rewards, dones, next_states, indexes, weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, env, buffer, net, target_net, device, batch_size, sync_every, discount_factor,\n",
    "                 learning_rate, discount_steps):\n",
    "        self.env = env\n",
    "        self.buffer = buffer\n",
    "        self.net = net\n",
    "        self.target_net = target_net\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_steps = sync_every\n",
    "        self.discount_steps = discount_steps\n",
    "        self.discount_factor = discount_factor\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "        self.writer = SummaryWriter(comment='-dqn-n-step-' + datetime.now().isoformat(timespec='seconds'))\n",
    "        self._reset()\n",
    "        self.episode_steps = EpisodeSteps(self.discount_steps)\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = self.env.reset()\n",
    "        self.total_episode_reward = 0\n",
    "\n",
    "    def train(self, target_reward):\n",
    "        step = 0\n",
    "        episode_rewards = []\n",
    "        while True:\n",
    "            self.optimizer.zero_grad()\n",
    "            episode_reward = self._play_single_step()\n",
    "\n",
    "            if len(self.buffer) < self.batch_size:\n",
    "                print('\\rFilling up the replay buffer...', end='')\n",
    "                continue\n",
    "\n",
    "            states, actions, rewards, dones, next_states, sample_indexes, sample_weights = self.buffer.sample(\n",
    "                self.batch_size)\n",
    "            loss, sample_priorities = self._calculate_loss(states, actions, next_states, dones, rewards, sample_weights)\n",
    "            self.buffer.update_priorities(sample_indexes, sample_priorities)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self._periodic_sync_target_network(step)\n",
    "\n",
    "            if episode_reward is not None:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                mean_reward = np.array(episode_rewards)[-100:].mean()\n",
    "                self._report_progress(step, loss.item(), episode_rewards, mean_reward)\n",
    "                if mean_reward > target_reward:\n",
    "                    print('\\nEnvironment Solved!')\n",
    "                    self.writer.close()\n",
    "                    break\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _play_single_step(self):\n",
    "        episode_reward = None\n",
    "        state_t = torch.FloatTensor(np.array([self.state], copy=False)).to(self.device)\n",
    "        q_actions = self.net(state_t)\n",
    "        action = torch.argmax(q_actions, dim=1).item()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        self.total_episode_reward += reward\n",
    "\n",
    "        self.episode_steps.append(self.state, action, reward, done, next_state)\n",
    "        if self.episode_steps.completed():\n",
    "            self.episode_steps.roll_out(discount_factor=self.discount_factor)\n",
    "            self.buffer.append(self.episode_steps)\n",
    "            self.episode_steps = EpisodeSteps(self.discount_steps)\n",
    "\n",
    "        if done:\n",
    "            episode_reward = self.total_episode_reward\n",
    "            self._reset()\n",
    "        else:\n",
    "            self.state = next_state\n",
    "        return episode_reward\n",
    "\n",
    "    def _calculate_loss(self, states, actions, next_states, dones, rewards, sample_weights):\n",
    "        state_q_all = self.net(states)\n",
    "        state_q_taken_action = state_q_all.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_q_all = self.target_net(next_states)\n",
    "            next_state_q_max = torch.max(next_state_q_all, dim=1)[0]\n",
    "            next_state_q_max[dones] = 0\n",
    "            state_q_expected = rewards + self.discount_factor * next_state_q_max\n",
    "            state_q_expected = state_q_expected.detach()\n",
    "        # PyTorch doesn't support weights for  MSELoss class\n",
    "        loss = (state_q_expected - state_q_taken_action) ** 2\n",
    "        weighted_loss = sample_weights * loss\n",
    "        return weighted_loss.mean(), (weighted_loss + 1e-6).data.cpu().numpy()\n",
    "\n",
    "    def _periodic_sync_target_network(self, step):\n",
    "        if step % self.sync_steps:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "    def _report_progress(self, step, loss, episode_rewards, mean_reward):\n",
    "        self.writer.add_scalar('Reward', mean_reward, step)\n",
    "        self.writer.add_scalar('loss', loss, step)\n",
    "        print(f'\\rsteps:{step} , episodes:{len(episode_rewards)}, loss: {loss:.6f} , '\n",
    "              f' reward: {mean_reward:.2f}', end='')\n",
    "\n",
    "    def demonstrate(self, net_state_file_path=None):\n",
    "        \"\"\"Demonstrate the performance of the trained net in a video\"\"\"\n",
    "        env = gym.wrappers.Monitor(self.env, 'videos', video_callable=lambda episode_id: True, force=True)\n",
    "        if net_state_file_path:\n",
    "            state_dict = torch.load(net_state_file_path, map_location=lambda stg, _: stg)\n",
    "            self.net.load_state_dict(state_dict)\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = self.net(torch.FloatTensor([state])).max(dim=1)[1]\n",
    "            new_state, reward, done, _ = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        print(\"Total reward: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1dba2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-91bb8c74812a12ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-91bb8c74812a12ce\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6988e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:668822 , episodes:1464, loss: 262748165035119345664.000000 ,  reward: -500.00000"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make(ENV_NAME)\n",
    "    buffer = PriorityReplayBuffer(capacity=REPLAY_BUFFER_CAPACITY, priority_alpha=PRIORITY_ALPHA,\n",
    "                                  priority_beta_start=PRIORITY_BETA_START,\n",
    "                                  priority_beta_grow_last_step=PRIORITY_BETA_GROW_LAST_STEP, device=DEVICE)\n",
    "    net = DuelingDQN(env.observation_space.shape[0], NETWORK_HIDDEN_SIZE, env.action_space.n).to(DEVICE)\n",
    "    target_net = DuelingDQN(env.observation_space.shape[0], NETWORK_HIDDEN_SIZE, env.action_space.n).to(DEVICE)\n",
    "    session = Session(env=env, buffer=buffer, net=net, target_net=target_net,\n",
    "                      device=DEVICE,\n",
    "                      batch_size=BATCH_SIZE, sync_every=SYNC_NETWORKS_EVERY_STEP, discount_factor=DISCOUNT_FACTOR,\n",
    "                      learning_rate=LEARNING_RATE, discount_steps=N_DISCOUNT_STEPS)\n",
    "    session.train(target_reward=DESIRED_TARGET_REWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0f3db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
